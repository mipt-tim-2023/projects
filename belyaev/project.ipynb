{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import typing\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "import itertools\n",
    "import functools\n",
    "import logging\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional\n",
    "import transformers\n",
    "import transformers.modeling_outputs\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def localize_globals(*exceptions: str, restore_values: bool = True):\n",
    "    exceptions: typing.Set[str] = set(exceptions)\n",
    "\n",
    "    old_globals: typing.Dict[str, typing.Any] = dict(globals())\n",
    "    allowed: typing.Set[str] = set(old_globals.keys())\n",
    "    allowed.update(exceptions)\n",
    "\n",
    "    yield None\n",
    "\n",
    "    new_globals: typing.Dict[str, typing.Any] = globals()\n",
    "\n",
    "    for name in tuple(new_globals.keys()):\n",
    "        if name not in allowed:\n",
    "            del new_globals[name]\n",
    "    \n",
    "    if not restore_values:\n",
    "        return\n",
    "    \n",
    "    new_globals.update(\n",
    "        {k: v for k, v in old_globals.items() if k not in exceptions}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[{levelname}] {message}\",\n",
    "    style=\"{\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>one_liner</th>\n",
       "      <th>long_description</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wufoo</td>\n",
       "      <td>Online form builder.</td>\n",
       "      <td>Wufoo is a web application that helps anybody ...</td>\n",
       "      <td>[SaaS, Productivity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Project Wedding</td>\n",
       "      <td></td>\n",
       "      <td>Finding wedding vendors is hard. In 2007, a co...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clustrix</td>\n",
       "      <td></td>\n",
       "      <td>Clustrix provides the leading scale-out relati...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inkling</td>\n",
       "      <td></td>\n",
       "      <td>Inkling, based in Chicago, Illinois, offers co...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audiobeta</td>\n",
       "      <td></td>\n",
       "      <td>AudioBeta develops web-based applications that...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name             one_liner  \\\n",
       "0            Wufoo  Online form builder.   \n",
       "1  Project Wedding                         \n",
       "2         Clustrix                         \n",
       "3          Inkling                         \n",
       "4        Audiobeta                         \n",
       "\n",
       "                                    long_description                  tags  \n",
       "0  Wufoo is a web application that helps anybody ...  [SaaS, Productivity]  \n",
       "1  Finding wedding vendors is hard. In 2007, a co...                    []  \n",
       "2  Clustrix provides the leading scale-out relati...                    []  \n",
       "3  Inkling, based in Chicago, Illinois, offers co...                    []  \n",
       "4  AudioBeta develops web-based applications that...                    []  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data: pd.DataFrame = pd.read_csv(\"yc_essential_data.csv\")\n",
    "\n",
    "# Limit to the columns we're interested in\n",
    "data = data[[\"name\", \"one_liner\", \"long_description\", \"tags\"]]\n",
    "\n",
    "# Convert tags to a list\n",
    "data[\"tags\"] = data[\"tags\"].apply(literal_eval)\n",
    "assert isinstance(data.at[0, \"tags\"], list), \"Didn't work!\"\n",
    "\n",
    "# Okay, apparently an empty string makes a nan by default\n",
    "# Gotta reverse it\n",
    "data[\"one_liner\"].replace(\n",
    "    to_replace=np.nan,\n",
    "    value=\"\",\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "data[\"long_description\"].replace(\n",
    "    to_replace=np.nan,\n",
    "    value=\"\",\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Preview the results\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          3D Printed Foods\n",
       "1               3D Printing\n",
       "2                        AI\n",
       "3              AI Assistant\n",
       "4      AI-Enhanced Learning\n",
       "               ...         \n",
       "324          Women's Health\n",
       "325     Workflow Automation\n",
       "326               eLearning\n",
       "327                 eSports\n",
       "328                    web3\n",
       "Length: 329, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather all unique tags\n",
    "with localize_globals(\"all_tags\"):\n",
    "    tags_set: set[str] = set(itertools.chain.from_iterable(data[\"tags\"]))\n",
    "    \n",
    "    all_tags: pd.Series = pd.Series(sorted(tags_set))\n",
    "\n",
    "all_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer: transformers.DistilBertTokenizer = transformers.DistilBertTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    ")\n",
    "\n",
    "nlp_model: transformers.DistilBertModel = transformers.DistilBertModel.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS: int = 512\n",
    "EMBEDDING_SIZE: int = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['name', 'one_liner', 'long_description', 'tags'],\n",
       "    num_rows: 4423\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with localize_globals(\"complete_dataset\", \"col_pad_len\"):\n",
    "    def preprocess(batch: dict[str, typing.Any]) -> dict[str, typing.Any]:\n",
    "        for column in (\n",
    "            \"name\",\n",
    "            \"one_liner\",\n",
    "            \"long_description\",\n",
    "        ):\n",
    "            tmp = tokenizer(\n",
    "                batch[column],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=MAX_TOKENS,\n",
    "                return_tensors=\"pt\",\n",
    "            ).data\n",
    "            \n",
    "            # logging.info(f\"!! {tmp['input_ids'].shape}, {tmp['attention_mask'].shape}\")\n",
    "            batch[column] = tmp[\"input_ids\"]\n",
    "            batch[f\"{column}_mask\"] = tmp[\"attention_mask\"]\n",
    "        \n",
    "        # TODO: Since this is the target, process it separately?\n",
    "        if \"tags\" in batch:\n",
    "            batch[\"tags\"] = torch.stack([\n",
    "                torch.tensor(all_tags.apply(tags.__contains__), dtype=torch.float)\n",
    "                for tags in batch[\"tags\"]\n",
    "            ])\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    complete_dataset = (\n",
    "        datasets.Dataset\n",
    "        .from_pandas(data)\n",
    "        # .with_format(None)\n",
    "        .with_transform(preprocess)\n",
    "    )\n",
    "\n",
    "complete_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['name', 'one_liner', 'long_description', 'tags'],\n",
       "     num_rows: 3538\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['name', 'one_liner', 'long_description', 'tags'],\n",
       "     num_rows: 619\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['name', 'one_liner', 'long_description', 'tags'],\n",
       "     num_rows: 266\n",
       " }))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with localize_globals(\"train_dataset\", \"val_dataset\", \"test_dataset\"):\n",
    "    train_test_split = complete_dataset.train_test_split(test_size=0.2)\n",
    "    train_dataset = train_test_split[\"train\"]\n",
    "    \n",
    "    test_val_split = train_test_split[\"test\"].train_test_split(test_size=0.3)\n",
    "    val_dataset = test_val_split[\"train\"]\n",
    "    test_dataset = test_val_split[\"test\"]\n",
    "\n",
    "train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes multiple inputs from named columns of a dataset,\n",
    "    passes them to separate sub-modules, and collects the\n",
    "    result with a single collector module.\n",
    "    \n",
    "    Note: the arguments are passed to the collector by\n",
    "    their order, the names are only used for column selection.\n",
    "    This behaviour relies on nn.ModuleDict preserving the\n",
    "    order of insertion, which should hold for Python >= 3.6.\n",
    "    If that's not the case, you'll get arbitrary but consistent (?)\n",
    "    order within a single `MultiInputModule` instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs: nn.ModuleDict\n",
    "    collector: nn.Module\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        inputs: nn.ModuleDict,\n",
    "        collector: nn.Module,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.collector = collector\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        # TODO: **?\n",
    "        input_dict: typing.Mapping[str, torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        assert set(input_dict.keys()).issuperset(self.inputs.keys()), \\\n",
    "            f\"Missing parameters: expected {set(self.inputs.keys())}, got only {set(input_dict.keys())}\"\n",
    "        \n",
    "        return self.collector(*(\n",
    "            self.inputs[name](input_dict[name])\n",
    "            for name in self.inputs\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenationModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a list of tensors and concatenates them\n",
    "    into a single tensor along a new axis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        *tensors: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        return torch.cat(tensors, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPWrapperModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps an nlp module and performs the following pre- and postprocessing:\n",
    "    - Takes a dictionary and `**`-unwraps it for the submodule's input\n",
    "    - Takes `.last_hidden_state` from the submodule's result and returns only it\n",
    "    \"\"\"\n",
    "    \n",
    "    submodule: nn.Module\n",
    "    \n",
    "    def __init__(self, submodule: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.submodule = submodule\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        params: typing.Mapping[str, torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        # logging.info(f\"! input_ids:{params['input_ids'].shape}, attention_mask:{params['attention_mask'].shape}\")\n",
    "        \n",
    "        return self.submodule(**params).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YCTagPredictorConfig(transformers.modeling_utils.PretrainedConfig):\n",
    "    model_type: typing.ClassVar[str] = \"yc_tag_predictor\"\n",
    "    \n",
    "    nlp_model: transformers.PreTrainedModel\n",
    "    \n",
    "    def __init__(self, nlp_model: transformers.PreTrainedModel, **kwargs: typing.Any) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.nlp_model = nlp_model\n",
    "\n",
    "\n",
    "class YCTagPredictorModel(transformers.modeling_utils.PreTrainedModel):\n",
    "    config_class = YCTagPredictorConfig\n",
    "    \n",
    "    def __init__(self, config: YCTagPredictorConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        \n",
    "        input_embedder: nn.Module = NLPWrapperModule(config.nlp_model)\n",
    "        \n",
    "        # I can't afford to also tune BERT, nor do I need to\n",
    "        input_embedder.train(False)\n",
    "        for param in input_embedder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            MultiInputModule(\n",
    "                inputs=nn.ModuleDict(dict(\n",
    "                    name=input_embedder,\n",
    "                    one_liner=input_embedder,\n",
    "                    long_description=input_embedder,\n",
    "                )),\n",
    "                collector=ConcatenationModule(),\n",
    "            ),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=EMBEDDING_SIZE * MAX_TOKENS * 3,\n",
    "                out_features=len(all_tags),\n",
    "            ),\n",
    "            # nn.Softmax(dim=-1),\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        *,\n",
    "        name: torch.Tensor,\n",
    "        name_mask: torch.Tensor,\n",
    "        one_liner: torch.Tensor,\n",
    "        one_liner_mask: torch.Tensor,\n",
    "        long_description: torch.Tensor,\n",
    "        long_description_mask: torch.Tensor,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.model(dict(\n",
    "            name=dict(\n",
    "                input_ids=name,\n",
    "                attention_mask=name_mask,\n",
    "            ),\n",
    "            one_liner=dict(\n",
    "                input_ids=one_liner,\n",
    "                attention_mask=one_liner_mask,\n",
    "            ),\n",
    "            long_description=dict(\n",
    "                input_ids=long_description,\n",
    "                attention_mask=long_description_mask,\n",
    "            ),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YCTagPredictorModel(\n",
    "    YCTagPredictorConfig(\n",
    "        nlp_model=nlp_model,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] actual_shape=torch.Size([329]), target_shape=torch.Size([329])\n"
     ]
    }
   ],
   "source": [
    "with localize_globals():\n",
    "    model.to(torch.device(\"cpu\"))\n",
    "    actual_shape = model(**next(train_dataset.iter(1)))[0].shape \n",
    "    target_shape = next(iter(train_dataset))[\"tags\"].shape\n",
    "    \n",
    "    logging.info(f\"{actual_shape=}, {target_shape=}\")\n",
    "    \n",
    "    assert actual_shape == target_shape, \"Bad model result shape\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./training_output\",\n",
    "    logging_dir=\"./training_logs\",\n",
    "    label_names=[\"tags\"],\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    remove_unused_columns=False,\n",
    "    # num_train_epochs=10,\n",
    "    # warmup_steps=100,\n",
    "    # weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(transformers.trainer.Trainer):\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: dict[str, typing.Any],\n",
    "        return_outputs: bool = False,\n",
    "    ) -> typing.Union[torch.Tensor, typing.Tuple[torch.Tensor, transformers.modeling_outputs.ModelOutput]]:\n",
    "        # print(\"!!!\", flush=True)\n",
    "        labels = inputs.pop(\"tags\")\n",
    "        outputs = model(**inputs)\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            outputs.view(-1), labels.view(-1),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer: transformers.trainer.Trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3538\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1329\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c66c68721f7408cb8434dcb1bf01de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.45 GiB (GPU 0; 8.00 GiB total capacity; 6.08 GiB already allocated; 185.00 MiB free; 6.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32md:\\Workspace\\.MIPT\\s7\\tim\\projects\\belyaev\\project.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Workspace/.MIPT/s7/tim/projects/belyaev/project.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\transformers\\trainer.py:1316\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1314\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1316\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1319\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1320\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1321\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1322\u001b[0m ):\n\u001b[0;32m   1323\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\transformers\\trainer.py:1867\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1865\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[0;32m   1866\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1867\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   1869\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.45 GiB (GPU 0; 8.00 GiB total capacity; 6.08 GiB already allocated; 185.00 MiB free; 6.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
