{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "513fge_wHqy6"
      },
      "outputs": [],
      "source": [
        "#insert here Youtube Api key\n",
        "API_KEY = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "PbKsNB0TIz5t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google_auth_oauthlib.flow\n",
        "import googleapiclient.discovery\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HhxfGiX8I3d0"
      },
      "outputs": [],
      "source": [
        "# Set up the YouTube API client\n",
        "youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey=API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QeKmr0U5Pds6"
      },
      "outputs": [],
      "source": [
        "from urllib.error  import HTTPError\n",
        "def search_videos_by_query(query_, results_num=100):\n",
        "  max_results = results_num\n",
        "  videos = []\n",
        "  next_page_token = None\n",
        "\n",
        "  print(query_)\n",
        "\n",
        "  while len(videos) < max_results:\n",
        "      request = youtube.search().list(\n",
        "          q=query_,\n",
        "          part='id,snippet',\n",
        "          type='video',\n",
        "          maxResults=min(50, max_results - len(videos)),  # Set the maximum results per page\n",
        "          pageToken=next_page_token\n",
        "      )\n",
        "      response = request.execute()\n",
        "      videos += response['items']\n",
        "      next_page_token = response.get('nextPageToken')\n",
        "\n",
        "      if not next_page_token:\n",
        "          break\n",
        "\n",
        "  return videos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yWOanfCTI8Dk"
      },
      "outputs": [],
      "source": [
        "# Search for videos related to Y Combinator applications\n",
        "def search_ycombinator_videos(batches = (\"W22\", \"S22\", \"W23\", \"S23\")):\n",
        "    res = {batch:[] for batch in batches}\n",
        "\n",
        "    for batch in batches:\n",
        "      for query_start in [\"YC application\",\n",
        "                          \"Y combinator application\",\n",
        "                          \"YC\",\n",
        "                          \"Y Combinator\"]:\n",
        "\n",
        "        query = query_start + \" \" + batch\n",
        "        videos = search_videos_by_query(query, results_num=200)\n",
        "        res[batch] += videos\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4uHpR9RZocl",
        "outputId": "c3cfbcfd-67c7-470c-a534-618bf872eb96"
      },
      "outputs": [],
      "source": [
        "# season_to_videos = search_ycombinator_videos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('season_to_videos.json', 'r') as json_file:\n",
        "    season_to_videos = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJck3Ty8Z30e",
        "outputId": "38ca00c9-eef2-4f9f-a591-98b58c8b3190"
      },
      "outputs": [],
      "source": [
        "videos = []\n",
        "for key, val in season_to_videos.items():\n",
        "    videos += val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "v0HMxfhAT1Yo"
      },
      "outputs": [],
      "source": [
        "def longest_common_substring(str1, str2):\n",
        "    m = len(str1)\n",
        "    n = len(str2)\n",
        "\n",
        "    # Create a table to store the lengths of common suffixes\n",
        "    table = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Variables to store the length and end position of the longest common substring\n",
        "    max_length = 0\n",
        "    end_position = 0\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if str1[i - 1] == str2[j - 1]:\n",
        "                table[i][j] = table[i - 1][j - 1] + 1\n",
        "                if table[i][j] > max_length:\n",
        "                    max_length = table[i][j]\n",
        "                    end_position = i - 1\n",
        "            else:\n",
        "                table[i][j] = 0\n",
        "\n",
        "    # Extract the longest common substring\n",
        "    longest_substring = str1[end_position - max_length + 1:end_position + 1]\n",
        "\n",
        "    return longest_substring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "U3bWPX-HQ6Mg"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "STOP_WORDS = [\"W22\",\n",
        "              \"S22\",\n",
        "              \"W23\",\n",
        "              \"S23\",\n",
        "              \"YC\",\n",
        "              \"Y\",\n",
        "              \"application\",\n",
        "              \"combinator\",\n",
        "              \"Application\",\n",
        "              \"Combinator\",\n",
        "              \"video\",\n",
        "              \"Video\"]\n",
        "\n",
        "def parse_batch(title, descr):\n",
        "  batches = (\"W22\", \"S22\", \"W23\", \"S23\")\n",
        "  for b in batches:\n",
        "    if b in (title + descr):\n",
        "      return b\n",
        "  \n",
        "  for year in (\"22\", \"23\"):\n",
        "    for season in (\"Summer\", \"Winter\", \"S\", \"W\"):\n",
        "      if year in title + descr and season in title + descr:\n",
        "        return season[0] + year\n",
        "  \n",
        "  return \"Unk\"\n",
        "\n",
        "\n",
        "def get_data_from_videos(videos):\n",
        "  processed_videos = {\"id\":[],\n",
        "                      \"title\":[],\n",
        "                      \"link\":[],\n",
        "                      \"description\":[],\n",
        "                      \"channel_title\":[],\n",
        "                      \"publish_time\":[],\n",
        "                      \"batch\":[]\n",
        "                      }\n",
        "  for video in videos:\n",
        "    if video['id']['videoId'] not in processed_videos[\"id\"]:\n",
        "      processed_videos[\"id\"].append(video['id']['videoId'])\n",
        "      processed_videos[\"link\"].append(\"https://www.youtube.com/watch?v=\" + video['id']['videoId'])\n",
        "      processed_videos[\"title\"].append(video['snippet']['title'])\n",
        "      processed_videos[\"description\"].append(video['snippet']['description'])\n",
        "      processed_videos[\"channel_title\"].append(video['snippet']['channelTitle'])\n",
        "      processed_videos[\"publish_time\"].append(video['snippet']['publishTime'])\n",
        "      processed_videos[\"batch\"].append(parse_batch(video['snippet']['title'], video['snippet']['description']))\n",
        "  return processed_videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnV02N7dUfle",
        "outputId": "72fca132-2506-4863-a8bf-2b66dbc73405"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "videos_data = pd.DataFrame(get_data_from_videos(videos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "yc_data = pd.read_json('combined_companies_data.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get successful applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_name(row):\n",
        "    if row['name'].lower() in row['all_info'].lower():\n",
        "        return True\n",
        "    \n",
        "    for name in row['former_names']:\n",
        "        if name.lower() in row['all_info'].lower():\n",
        "            return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "videos_data[\"all_info\"] = videos_data.apply(lambda row: ' '.join([row[\"title\"], row[\"channel_title\"]]), axis=1)\n",
        "videos_data['join'] = 1\n",
        "yc_data['join'] = 1\n",
        "success_applications = pd.merge(videos_data, yc_data, on='join', how='inner')\n",
        "success_applications['is_substr'] = success_applications.apply(check_name, axis=1)\n",
        "result = success_applications[success_applications['is_substr']]\n",
        "result = result.drop(columns=['is_substr', 'join'])\n",
        "result = result[result['name'] != 'Y Combinator']\n",
        "result = result[result['channel_title'] != 'Y Combinator']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 401,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = result[result['batch_x'] == result['batch_y']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 405,
      "metadata": {},
      "outputs": [],
      "source": [
        "result.to_excel(\"accepted_videos.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get unsuccessful applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "accepted = set(result['channel_title'].to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_accepted = videos_data[videos_data['channel_title'].apply(lambda x: x not in accepted)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_accepted = not_accepted[not_accepted['batch'] != 'Unk']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "words_to_remove = [\"yc\",\n",
        "                   \"combinator\",\n",
        "                   \"y\",\n",
        "                   \"c\",\n",
        "                   \"demo\",\n",
        "                   \"video\",\n",
        "                   \"application\",\n",
        "                   \"for\",\n",
        "                   \"founders\",\n",
        "                   \"team\",\n",
        "                   \"pitch\"]\n",
        "for season in [\"s\", \"w\", \"\", \"summer\", \"winter\"]:\n",
        "    for year in [\"22\", \"23\", \"2022\", \"2023\", \"\"]:\n",
        "        words_to_remove.append(season+year)\n",
        "\n",
        "def proces_title(title):\n",
        "    title = ''.join([s for s in title if s not in string.punctuation])\n",
        "    company_name = []\n",
        "    for word in title.split():\n",
        "        if word.lower() not in words_to_remove and len(word) >= 3:\n",
        "            company_name.append(word)\n",
        "    if len(company_name) > 3:\n",
        "        return \"\"\n",
        "    return ' '.join(company_name)\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_accepted['name'] = not_accepted['title'].apply(proces_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_accepted = not_accepted[not_accepted.apply(lambda row: len(row['name']) >= 3, axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_accepted.to_excel(\"not_accepted.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not accepted with website in description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_accepted = not_accepted[not_accepted.apply(lambda row: \".com\" in row['description'] or \"www\" in row['description'] or \"http\" in row['description'], axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_accepted.to_excel(\"not_accepted_with_website.xlsx\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
